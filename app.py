import streamlit as st
from modules import gemini_brain, nexus_bridge, ui_components

st.set_page_config(page_title="AURA TRINITY", layout="wide")
ui_components.setup_style()

# --- [ì‚¬ì´ë“œë°” ì„¤ì •] ---
with st.sidebar:
    st.title("ğŸ›ï¸ AURA CONTROL")
    
    st.subheader("ğŸ§  AI Brain Select")
    model_option = st.selectbox(
        "ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
        (
            "gemini-3-flash-preview",    # [ê³µì‹] Gemini 3 Flash (ë¬´ë£Œ í‹°ì–´ ì§€ì›)
            "gemini-3-pro-preview",      # [ê³µì‹] Gemini 3 Pro (ìœ ë£Œ ê°€ëŠ¥ì„± ìˆìŒ)
            "gemini-2.0-flash-exp",      # 2.0 ì‹¤í—˜ ë²„ì „
            "gemini-1.5-flash"           # êµ¬ë²„ì „ (ë¹„ìƒìš©)
        ),
        index=0 # ê¸°ë³¸ê°’ì„ ë¬´ë£Œì¸ 'Gemini 3 Flash'ë¡œ ì„¤ì •
    )
    
    # ëª¨ë¸ëª… ì§ì ‘ ì…ë ¥ ê¸°ëŠ¥ (í˜¹ì‹œ ëª¨ë¥¼ ìƒí™© ëŒ€ë¹„)
    use_custom = st.checkbox("ì§ì ‘ ëª¨ë¸ëª… ì…ë ¥")
    if use_custom:
        model_option = st.text_input("ëª¨ë¸ëª… ì…ë ¥", value=model_option)

    st.info(f"ì„ íƒëœ ë‘ë‡Œ: {model_option}")
    st.markdown("---")
    
    gist_url = st.text_input("ğŸ”— Gist Raw URL")
    if st.button("ğŸ”„ ì‹œìŠ¤í…œ ì¬ê°€ë™"):
        st.rerun()

# --- [ë©”ì¸ ë¡œì§] ---
if "gemini_ready" not in st.session_state:
    st.session_state.gemini_ready = gemini_brain.init_gemini()

col1, col2 = st.columns([1, 1.2])

with col1:
    st.subheader("ğŸ“¡ LOCAL AGENT LOG")
    if gist_url:
        log_data = nexus_bridge.get_nexus_log(gist_url)
        ui_components.render_log_box(log_data)
    else:
        st.info("ğŸ‘ˆ Gist URLì„ ì…ë ¥í•˜ì„¸ìš”.")

with col2:
    st.subheader("ğŸ’¬ COMMAND CENTER")
    
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    if prompt := st.chat_input("ëª…ë ¹ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        if st.session_state.gemini_ready:
            with st.chat_message("assistant"):
                with st.spinner(f"Thinking with {model_option}..."):
                    # [í•µì‹¬] ì„ íƒí•œ model_optionì„ ì „ë‹¬
                    history_context = [{"role": m["role"], "parts": [m["content"]]} for m in st.session_state.messages if m["role"] != "system"]
                    response = gemini_brain.get_response(history_context, prompt, model_name=model_option)
                    st.write(response)
            
            st.session_state.messages.append({"role": "assistant", "content": response})
